# Temporal Context RAG Agent

A production-ready Retrieval-Augmented Generation (RAG) system with temporal context awareness, table-aware chunking, and comprehensive citation tracking. Built with Google Cloud Vertex AI and FastAPI.

## Overview

This system provides advanced document processing and semantic search with:
- **Temporal Context Awareness**: Automatic date extraction and temporal enhancement of embeddings
- **Table-Aware Chunking**: Intelligent text chunking that preserves table integrity
- **GCS Import**: Directly import documents from Google Cloud Storage without re-uploading
- **Citation Tracking**: Comprehensive citations with clickable links and metadata
- **Production-Ready**: Vector Search with configurable algorithms (BruteForce/TreeAH)

---

## Quick Start

### Prerequisites
- Python 3.9+, Node.js 16+
- Google Cloud Project with enabled APIs: Vertex AI, Vector Search, Cloud Storage

### 1. Backend Setup

```bash
cd backend

# Setup environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Configure .env
cp .env.example .env
# Edit with your credentials
```

**Required `.env` variables:**
```env
GOOGLE_CLOUD_PROJECT=your-project-id
GOOGLE_CLOUD_LOCATION=us-central1
VERTEX_AI_CORPUS_NAME=temporal-rag-corpus
GCS_BUCKET_NAME=your-bucket-name
EMBEDDING_MODEL_NAME=text-embedding-005
```

### 2. Start Services

```bash
# Authenticate
gcloud auth application-default login

# Start backend
python main.py  # http://localhost:8000

# Start frontend (new terminal)
cd frontend
npm install
npm start  # http://localhost:3000
```

### 3. Quick Test

**Chat Interface:**
```
1. Create a corpus for financial documents
2. Upload a PDF about Q4 2023
3. Query: "What was the revenue in Q4 2023?"
```

**API:**
```bash
# Create index
curl -X POST http://localhost:8000/index/create \
  -H "Content-Type: application/json" \
  -d '{"description": "Test index", "dimensions": 768}'

# Upload document
curl -X POST http://localhost:8000/documents/upload \
  -F "file=@report.pdf" \
  -F "document_date=2023-12-31"

# Query
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Q4 2023 revenue", "top_k": 5}'
```

---

## Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (React + Material-UI)                              â”‚
â”‚  Chat | Query | Corpus Manager | Document Importer           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Backend                                              â”‚
â”‚  â€¢ Document upload/GCS import â€¢ Text processing               â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚            â”‚              â”‚               â”‚
   â–¼            â–¼              â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Parserâ”‚  â”‚ Chunker  â”‚  â”‚Embeddingsâ”‚  â”‚ Vector Search    â”‚
â”‚ PDF  â”‚  â”‚Table-    â”‚  â”‚Temporal  â”‚  â”‚BruteForce/TreeAH â”‚
â”‚DOCX  â”‚  â”‚Aware     â”‚  â”‚Enhanced  â”‚  â”‚DOT_PRODUCT       â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

**Backend Components:**
- `main.py`: FastAPI REST API with CORS
- `vector_search_manager.py`: Vector Search operations, GCS import, citation generation
- `temporal_embeddings.py`: Date extraction, temporal context enhancement
- `text_chunker.py`: Table-aware chunking with quality scoring
- `document_parser.py`: Multi-format text extraction (PDF, DOCX, TXT, MD)

**Frontend Components:**
- `ChatInterface.js`: Conversational UI with citations
- `QueryInterface.js`: Search UI with rich result cards
- `DocumentImporter.js`: File upload and GCS import UI
- `CorpusManager.js`: Index/endpoint management

---

## Pipeline Flow Diagrams

### Complete Document Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DOCUMENT INGESTION                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   File Upload       â”‚       â”‚   GCS Import         â”‚
         â”‚   (Frontend)        â”‚       â”‚   (gs://bucket/...)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DOCUMENT PARSING                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚    PDF      â”‚    DOCX     â”‚    TXT      â”‚     MD      â”‚             â”‚
â”‚  â”‚ pdfplumber  â”‚  python-    â”‚   UTF-8     â”‚  Markdown   â”‚             â”‚
â”‚  â”‚ + pypdf     â”‚   docx      â”‚  Decode     â”‚  Preserved  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚        â”‚             â”‚             â”‚             â”‚                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                â”‚
â”‚   â”‚   TABLE DETECTION & EXTRACTION (PDF Only)          â”‚                â”‚
â”‚   â”‚   â€¢ Find tables using pdfplumber.find_tables()     â”‚                â”‚
â”‚   â”‚   â€¢ Validate: â‰¥2 rows, â‰¥2 columns, valid bbox     â”‚                â”‚
â”‚   â”‚   â€¢ Convert to markdown: [TABLE N]...[END TABLE]   â”‚                â”‚
â”‚   â”‚   â€¢ Extract text OUTSIDE table regions (anti-dup)  â”‚                â”‚
â”‚   â”‚   â€¢ Maintain document order by y-position          â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TEMPORAL CONTEXT EXTRACTION                           â”‚
â”‚  â€¢ Extract dates: YYYY-MM-DD, MM/DD/YYYY, Month Day Year               â”‚
â”‚  â€¢ Extract fiscal periods: Q1 2023, FY2023, H1 2024                    â”‚
â”‚  â€¢ Extract from filenames: "July 1st. 2025.PDF" â†’ 2025-07-01          â”‚
â”‚  â€¢ Table-aware: Detect temporal data in tables (with column context)   â”‚
â”‚  â€¢ Deduplicate: Remove overlapping temporal references                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TABLE-AWARE CHUNKING                                â”‚
â”‚  1. Extract table positions: [TABLE N] markers                         â”‚
â”‚  2. Segment text: [text] [table] [text] [table] ...                    â”‚
â”‚  3. Chunk text segments: Hierarchical (headers â†’ paragraphs â†’ sentences)â”‚
â”‚  4. Keep tables ATOMIC: Never split mid-table                          â”‚
â”‚  5. Table-aware overlap: No overlap across table boundaries            â”‚
â”‚  6. Quality scoring: Different criteria for text vs table chunks       â”‚
â”‚                                                                          â”‚
â”‚  Output: List of chunks with metadata                                   â”‚
â”‚    â€¢ content, page_number, chunk_index, page_chunk_index               â”‚
â”‚    â€¢ has_table, table_count, has_complete_table                        â”‚
â”‚    â€¢ quality_score (0-1), sentence_count, word_count                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TEMPORAL EMBEDDING ENHANCEMENT                         â”‚
â”‚  For each chunk:                                                        â”‚
â”‚    1. Extract temporal entities from chunk text                        â”‚
â”‚    2. Build temporal context prefix (max 200 chars):                   â”‚
â”‚       "[TEMPORAL_CONTEXT: Document Date: 2023-12-31 |                  â”‚
â”‚        Contains Table Data | Fiscal Quarters (Tabular): Q1 2023 |      â”‚
â”‚        Dates: 2023-01-15 | Years: 2023]"                               â”‚
â”‚    3. Prepend to chunk text                                            â”‚
â”‚    4. Generate embedding using Vertex AI (text-embedding-005)          â”‚
â”‚    5. Rate limit: 60 req/min, retry with exponential backoff           â”‚
â”‚                                                                          â”‚
â”‚  Output: 768-dimensional vectors with temporal awareness               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STORAGE & INDEXING                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Google Cloud Storage    â”‚      â”‚  Vertex AI Vector Search        â”‚ â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚ â”‚
â”‚  â”‚  Original File (optional)â”‚      â”‚  â€¢ Upsert embeddings + metadata â”‚ â”‚
â”‚  â”‚  Chunk JSON (required)   â”‚      â”‚  â€¢ DOT_PRODUCT_DISTANCE         â”‚ â”‚
â”‚  â”‚  â””â”€ Enhanced chunks      â”‚      â”‚  â€¢ BruteForce or TreeAH index   â”‚ â”‚
â”‚  â”‚  â””â”€ Metadata             â”‚      â”‚  â€¢ Deployed on e2-standard-2    â”‚ â”‚
â”‚  â”‚  â””â”€ Quality scores       â”‚      â”‚  â€¢ find_neighbors API           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                             QUERY PIPELINE
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER QUERY                                       â”‚
â”‚  "What was the Q4 2023 revenue?" + optional temporal filters            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  QUERY TEMPORAL ENHANCEMENT                              â”‚
â”‚  1. Extract temporal entities from query                                â”‚
â”‚  2. Build temporal context (same as document processing)                â”‚
â”‚  3. Generate query embedding (768-dim)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VECTOR SIMILARITY SEARCH                               â”‚
â”‚  1. Call index_endpoint.find_neighbors(query_embedding)                â”‚
â”‚  2. Apply temporal filters (post-retrieval) if specified               â”‚
â”‚  3. Sort by DOT_PRODUCT score (higher = more similar)                  â”‚
â”‚  4. Return top_k results (default: 10)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CITATION GENERATION                                 â”‚
â”‚  For each result:                                                       â”‚
â”‚    â€¢ document_id, title, source (filename)                             â”‚
â”‚    â€¢ score, relevance (0.0-1.0+)                                       â”‚
â”‚    â€¢ page_number, chunk_index, page_chunk_index                        â”‚
â”‚    â€¢ quality_score (from chunking)                                     â”‚
â”‚    â€¢ original_file_url, clickable_link                                 â”‚
â”‚    â€¢ document_date (YYYY-MM-DD)                                        â”‚
â”‚    â€¢ formatted citation with all metadata                              â”‚
â”‚                                                                          â”‚
â”‚  Format: "Title (Page N, Chunk M) | Date: YYYY-MM-DD |                 â”‚
â”‚           Relevance: 0.XX | Source: filename.pdf"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RESULTS DISPLAY                                   â”‚
â”‚  Rich cards with:                                                       â”‚
â”‚    â€¢ Content snippet                                                    â”‚
â”‚    â€¢ Citation with clickable link                                      â”‚
â”‚    â€¢ Metadata chips (page, quality, date)                              â”‚
â”‚    â€¢ Relevance score badge                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Table Extraction Detailed Flow (PDF Only)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PDF PAGE PROCESSING                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                                â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  1. TABLE DETECTION     â”‚      â”‚  2. TEXT EXTRACTION    â”‚
      â”‚  pdfplumber.find_tables â”‚      â”‚  (Done in parallel)    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                 â”‚
               â–¼                                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
      â”‚  For each table_obj:    â”‚               â”‚
      â”‚  â€¢ Validate bbox        â”‚               â”‚
      â”‚    (not None, lenâ‰¥4)    â”‚               â”‚
      â”‚  â€¢ Extract table data   â”‚               â”‚
      â”‚  â€¢ Filter empty rows    â”‚               â”‚
      â”‚  â€¢ Validate structure:  â”‚               â”‚
      â”‚    - â‰¥2 rows            â”‚               â”‚
      â”‚    - â‰¥2 columns         â”‚               â”‚
      â”‚  â€¢ Normalize columns    â”‚               â”‚
      â”‚  â€¢ Convert to markdown  â”‚               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
               â”‚                                 â”‚
               â–¼                                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
      â”‚  Valid tables ONLY:     â”‚               â”‚
      â”‚  â€¢ [TABLE N] marker     â”‚               â”‚
      â”‚  â€¢ Markdown table       â”‚               â”‚
      â”‚  â€¢ [END TABLE] marker   â”‚               â”‚
      â”‚  â€¢ Store bbox for       â”‚               â”‚
      â”‚    text filtering       â”‚               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
               â”‚                                 â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  3. ANTI-DUPLICATION FILTER  â”‚
               â”‚  â€¢ Extract text in bands:    â”‚
               â”‚    - Before first table      â”‚
               â”‚    - Between tables          â”‚
               â”‚    - After last table        â”‚
               â”‚  â€¢ Filter chars by position: â”‚
               â”‚    EXCLUDE if inside bbox    â”‚
               â”‚  â€¢ Prevents text duplication â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  4. POSITION-BASED ASSEMBLY  â”‚
               â”‚  â€¢ Sort by y-coordinate      â”‚
               â”‚  â€¢ Interleave text & tables  â”‚
               â”‚  â€¢ Join with \n\n            â”‚
               â”‚  â€¢ Preserve document order   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  PAGE TEXT       â”‚
                    â”‚  (Order preservedâ”‚
                    â”‚   No duplication)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GCS Import vs File Upload Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FILE UPLOAD                â”‚         GCS IMPORT                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                    â”‚                                    â”‚
â”‚  User selects file from computer   â”‚  User provides GCS path            â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  Frontend uploads to backend       â”‚  Backend reads from GCS            â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  Backend receives bytes            â”‚  Backend downloads bytes           â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  Parse â†’ Chunk â†’ Embed             â”‚  Parse â†’ Chunk â†’ Embed             â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  STORE ORIGINAL FILE TO GCS âœ—      â”‚  SKIP STORING ORIGINAL âœ“           â”‚
â”‚  (Duplicate storage ~50MB)         â”‚  (Use existing file)               â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  Store chunk JSON to GCS âœ“         â”‚  Store chunk JSON to GCS âœ“         â”‚
â”‚  (~1.25MB for 125 chunks)          â”‚  (~1.25MB for 125 chunks)          â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  Upsert vectors to index âœ“         â”‚  Upsert vectors to index âœ“         â”‚
â”‚         â†“                          â”‚         â†“                          â”‚
â”‚  Metadata includes:                â”‚  Metadata includes:                â”‚
â”‚    â€¢ gcs_chunk_url                 â”‚    â€¢ original_file_url âœ“           â”‚
â”‚    â€¢ source_url                    â”‚    â€¢ gcs_source_path âœ“             â”‚
â”‚    â€¢ uploaded_via: "upload"        â”‚    â€¢ gcs_chunk_url                 â”‚
â”‚                                    â”‚    â€¢ imported_from_gcs: true       â”‚
â”‚                                    â”‚                                    â”‚
â”‚  Storage: ~51.25MB per doc         â”‚  Storage: ~1.25MB per doc          â”‚
â”‚  Efficiency: â­â­                   â”‚  Efficiency: â­â­â­â­â­              â”‚
â”‚  Best for: Small batches           â”‚  Best for: Bulk import             â”‚
â”‚                                    â”‚           Existing GCS files       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        STORAGE SAVINGS EXAMPLE
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  100 PDFs @ 50MB each                               â”‚
        â”‚                                                      â”‚
        â”‚  File Upload:   5,125 MB (5.0 GB)                   â”‚
        â”‚  GCS Import:      125 MB (0.12 GB)                  â”‚
        â”‚                                                      â”‚
        â”‚  Savings:       5,000 MB (4.88 GB) - 97.6% less!   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Features

### 1. GCS Import (No Duplicate Storage)

Import documents directly from Google Cloud Storage:

```javascript
// Frontend API call
import { importFromGCS } from './api';

await importFromGCS(
  'gs://my-bucket/documents/',
  '2023-12-31',  // Optional document_date
  true           // recursive
);
```

**Storage Optimization:**
- âœ… Original files NOT re-uploaded (uses existing GCS URLs)
- âœ… Chunk JSON created (processed output with metadata)
- âœ… **Savings**: ~5GB per 100 large files

**Process Flow:**
```
gs://bucket/file.pdf (50MB)
  â†“ Download bytes
  â†“ Parse (page-by-page for PDFs, table detection)
  â†“ Chunk (1000 chars, 200 overlap, table-aware)
  â†“ Enhance with temporal context
  â†“ Generate embeddings (768-dim, batched)
  â†“ Store chunk JSON (~1.25MB for 125 chunks) âœ…
  â†“ Upsert vectors to index âœ…
  â†“ Save metadata âœ…
```

**Metadata for GCS Imports:**
```json
{
  "original_file_url": "https://storage.cloud.google.com/existing-bucket/file.pdf",
  "gcs_source_path": "gs://existing-bucket/documents/file.pdf",
  "imported_from_gcs": true,
  "chunk_json_url": "https://storage.cloud.google.com/my-bucket/vector_search/.../chunk.json",
  "page_number": 2,
  "chunk_index": 42,
  "quality_score": 0.85
}
```

### 2. Temporal Context Enhancement

Automatic date extraction and embedding enhancement:

```python
# Original text
"Q4 2023 revenue was $10M in December"

# Enhanced text (before embedding)
"[TEMPORAL_CONTEXT: Document Date: 2023-12-31 | Contains dates: December 2023 | Years: 2023]
Q4 2023 revenue was $10M in December"
```

**Supported Date Formats:**
- ISO: `2023-12-31`
- US: `12/31/2023`
- Natural: `December 2023`, `Q4 2023`, `FY2023`
- Fiscal periods, quarters

### 3. Table-Aware Chunking

Intelligent chunking that preserves table integrity:

```
[TABLE 1]
| Quarter | Revenue | Growth |
|---------|---------|--------|
| Q4 2023 | $10M    | 15%    |
[END TABLE]

Standard text continues here...
```

**Features:**
- Tables kept intact (not split mid-content)
- Large tables get dedicated chunks
- Quality scoring: 0-1 based on completeness, sentence boundaries
- Page tracking for PDFs
- Metadata: `has_table`, `table_count`, `has_complete_table`

### 4. Citation System

Comprehensive citations with all query results:

```json
{
  "content": "Q4 2023 revenue increased...",
  "score": 0.8756,
  "citation": {
    "title": "Q4 2023 Report",
    "filename": "report.pdf",
    "original_file_url": "https://storage.cloud.google.com/.../report.pdf",
    "page_number": 2,
    "chunk_index": 42,
    "page_chunk_index": 6,
    "quality_score": 0.85,
    "document_date": "2023-12-31",
    "imported_from_gcs": true
  }
}
```

**Frontend Display:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ“„ Q4 2023 Report         [Relevance: 87.56%]â”‚
â”‚                                             â”‚
â”‚ "Q4 2023 revenue increased by 15%..."     â”‚
â”‚                                             â”‚
â”‚ ðŸ“ Page 2, Chunk 6  â­ Quality: 85%        â”‚
â”‚ ðŸ“… 2023-12-31      ðŸ“Ž report.pdf           â”‚
â”‚                                             â”‚
â”‚ [ðŸ”— View Source]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API Reference

### Index Management

```bash
# Create index
POST /index/create
{
  "description": "Financial documents index",
  "dimensions": 768,
  "index_algorithm": "brute_force"  # or "tree_ah"
}

# Get index info
GET /index/info

# Clear datapoints
POST /index/clear

# Delete infrastructure
DELETE /index/delete
```

### Document Operations

```bash
# Upload file
POST /documents/upload
Content-Type: multipart/form-data
- file: PDF/DOCX/TXT/MD
- document_date: 2023-12-31 (optional)
- chunk_size: 1000 (optional)
- chunk_overlap: 200 (optional)

# Import from GCS
POST /documents/import_from_gcs
Content-Type: multipart/form-data
- gcs_path: gs://bucket/path/ or gs://bucket/file.pdf
- document_date: 2023-12-31 (optional)
- recursive: true (optional)

# Manual import
POST /documents/import
{
  "documents": [{
    "content": "Text content",
    "metadata": {
      "title": "Document Title",
      "document_date": "2023-12-31",
      "source_url": "https://example.com/doc.pdf"
    }
  }],
  "bucket_name": "optional-gcs-bucket"
}

# Get document by ID
GET /documents/{document_id}
```

### Querying

```bash
# Semantic search
POST /query
{
  "query": "What was Q4 2023 revenue?",
  "top_k": 5,
  "temporal_filter": {
    "document_date": "2023-12-31"
  }
}

# Chat
POST /chat
{
  "message": "User message",
  "conversation_history": [...],
  "session_id": "optional-session-id",
  "user_id": "default_user"
}

# Extract temporal entities
POST /temporal/extract
{
  "text": "Revenue was $10M in Q4 2023"
}
```

---

## Development

### Project Structure

```
agent-temporal-context/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                     # FastAPI REST API
â”‚   â”œâ”€â”€ agent.py                    # AI agent orchestration and query processing
â”‚   â”œâ”€â”€ vector_search_manager.py    # Vector Search + GCS import
â”‚   â”œâ”€â”€ temporal_embeddings.py      # Temporal enhancement
â”‚   â”œâ”€â”€ text_chunker.py             # Table-aware chunking
â”‚   â”œâ”€â”€ document_parser.py          # Multi-format parsing
â”‚   â”œâ”€â”€ config.py                   # Settings (pydantic-settings)
â”‚   â”œâ”€â”€ logging_config.py           # Centralized logging
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.js        # Chat UI
â”‚   â”‚   â”‚   â”œâ”€â”€ QueryInterface.js       # Search UI
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentImporter.js     # Upload + GCS import
â”‚   â”‚   â”‚   â””â”€â”€ CorpusManager.js        # Index management
â”‚   â”‚   â”œâ”€â”€ App.js                      # Main app
â”‚   â”‚   â””â”€â”€ api.js                      # API client
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

### Adding New API Endpoints

1. Add method to `vector_search_manager.py`:
```python
async def new_method(self, params):
    # Implementation
    return {"success": True, "data": result}
```

2. Add API endpoint in `main.py`:
```python
@app.post("/new_endpoint")
async def new_endpoint(request: NewRequest):
    result = await vector_search_manager.new_method(...)
    return {"success": True, "data": result}
```

3. Update frontend API client in `api.js`:
```javascript
export const newMethod = async (params) => {
  const response = await api.post('/new_endpoint', params);
  return response.data;
};
```

### Testing

```bash
# Backend
cd backend
pytest tests/

# Frontend
cd frontend
npm test
npm build  # Production build
```

### Docker Deployment

```bash
# Create .env with credentials
docker-compose up --build

# Access
# Frontend: http://localhost:3000
# Backend: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

---

## Configuration

### Embedding Models

Supported Vertex AI models:

| Model | Dimensions | Use Case |
|-------|-----------|----------|
| `text-embedding-005` | 768 | Latest, best quality (default) |
| `text-embedding-004` | 768 | Previous generation |
| `textembedding-gecko@003` | 768 | Legacy/stable |

Configure via `EMBEDDING_MODEL_NAME` in `.env`.

### Index Algorithms

| Algorithm | Best For | Characteristics |
|-----------|----------|-----------------|
| `brute_force` | <10K docs | Fast queries, exact results |
| `tree_ah` | >10K docs | Approximate results, production scale |

### Chunking Configuration

```python
# Configurable per upload
chunk_size = 1000       # Characters per chunk
chunk_overlap = 200     # Overlap between chunks

# Default in TextChunker
TextChunker(
    chunk_size=1000,
    chunk_overlap=200,
    respect_structure=True  # Respect markdown/structure
)
```

---

## Troubleshooting

### Backend Issues

**Authentication:**
```bash
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID
```

**Vector Search Errors:**
- Enable APIs: Vertex AI, Vector Search, Cloud Storage
- Index deployment takes 5-10 minutes
- Check quotas in GCP Console

**Embedding Errors:**
- Verify model name in `.env`
- Check Vertex AI API quota
- Ensure project has access to embedding models

### Frontend Issues

**CORS Errors:**
- Check `main.py` CORS configuration
- Verify `REACT_APP_API_URL` in frontend `.env`

**Citation Links Not Working:**
- Ensure documents have proper metadata
- Check GCS bucket permissions (public read or authenticated)
- Verify bucket name matches `.env`

### GCS Import Issues

**Path Not Found:**
- Verify `gs://bucket/path` format
- Check GCS bucket exists and is accessible
- Ensure files are supported types: pdf, docx, txt, md

**Import Fails:**
- Check file parsing errors in backend logs
- Verify sufficient GCS permissions (read access)
- Ensure embedding API quota available

---

## Best Practices

### Document Import
1. **Always provide metadata**: `title`, `document_date`, `source_url`
2. **Use GCS import for bulk**: Import entire folders recursively
3. **Add document dates**: Enables temporal filtering
4. **Descriptive titles**: Improves citation display

### Querying
1. **Natural language**: Write queries as questions
2. **Temporal filters**: Use for time-sensitive searches
3. **Adjust top_k**: 5-10 results recommended
4. **Review scores**: >0.7 = strong match, 0.5-0.7 = moderate

### Citations
1. **Include source_url**: For external documents
2. **Check clickable_link**: Before displaying in UI
3. **Open in new tab**: Better UX for source viewing
4. **Show metadata**: Page numbers, quality scores add context

### Performance
1. **Batch operations**: Import multiple documents together
2. **Use tree_ah**: For >10K documents
3. **Monitor quotas**: Embedding API, Vector Search
4. **Cache results**: Frontend caching for repeated queries

---

## Technologies

**Backend:**
- Python 3.9+, FastAPI
- Google Cloud Vertex AI (Embeddings, Vector Search)
- Google Cloud Storage
- Pydantic Settings

**Frontend:**
- React 18
- Material-UI (MUI)
- Axios

**AI Models:**
- text-embedding-005 (768 dimensions)

---

## License

MIT

## Support

- **API Docs**: http://localhost:8000/docs (Swagger/OpenAPI)
- **Logs**: Backend terminal + Browser DevTools Console
- **Issues**: Check GitHub repository
- **GCP Console**: Monitor Vector Search, quotas, storage

---

## Summary

This system provides a complete RAG solution with:
- âœ… **Temporal awareness** via date extraction and enhanced embeddings
- âœ… **Text processing** from PDF, DOCX, TXT, and Markdown files
- âœ… **GCS import** for efficient bulk document loading without re-uploading
- âœ… **Table-aware chunking** preserving data integrity
- âœ… **Comprehensive citations** with page numbers, quality scores, and metadata
- âœ… **Production-ready** Vector Search with configurable algorithms (BruteForce/TreeAH)
- âœ… **Modern React UI** with rich result display

Perfect for building intelligent document search systems with time-sensitive information, structured data (tables), and comprehensive source tracking.
